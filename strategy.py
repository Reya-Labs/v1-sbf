from abc import ABCMeta, abstractmethod
import stat
from matplotlib.cbook import Stack
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from event import SignalEvent
import pandas as pd
import os
import statsmodels.api as sm
from statsmodels.regression.linear_model import OLS
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import coint
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import RANSACRegressor

SECONDS_IN_YEAR = 31536000

class Strategy(object):
    """
    Strategy is an abstract base class providing an interface for
    all subsequent (inherited) strategy handling objects.

    The goal of a (derived) Strategy object is to generate Signal
    objects for particular tokens based on the inputs of rates
    generated by a DataHandler object.

    This is designed to work both with historic and live data as
    the Strategy object is agnostic to the data source,
    since it obtains the rate tuples from a queue object.
    """

    __metaclass__ = ABCMeta

    @abstractmethod
    def calculate_signals(self, event):
        """
        Provides the mechanisms to calculate the list of signals.
        """
        raise NotImplementedError("Should implement calculate_signals()")

class LongRateStrategy(Strategy):
    """
    This is an extremely simple strategy that goes LONG (VT) all of the
    tokens as soon as a rate is received. It will never exit a position.

    It is primarily used as a testing mechanism for the Strategy class
    as well as a benchmark upon which to compare other strategies.
    """

    def __init__(self, rates, events):
        """
        Initialises the long rate strategy.

        Parameters:
        rates - The DataHandler object that provides rate information
        events - The Event Queue object.
        """
        self.rates = rates
        self.token_list = self.rates.token_list
        self.events = events

        # Once long (vt) & hold signal is given, these are set to True
        self.aped = self._calculate_initial_aped()

    def _calculate_initial_aped(self):
        """
        Adds keys to the aped dictionary for all tokens
        and sets them to False.
        """
        aped = {}
        for t in self.token_list:
            aped[t] = False
        return aped

    def calculate_signals(self, event):
        """
        For "VT and Hold" we generate a single signal per token
        and then no additional signals. This means we are
        constantly long the underlying rates market
        from the date of strategy initialisation.

        Parameters
        event - A MarketEvent object
        """
        if event.type == 'MARKET':
            for t in self.token_list:
                rates = self.rates.get_latest_rates(t, N=1)
                if rates is not None and rates != []:
                    if not self.aped[t]:
                        # (Token, Direction = LONG, SHORT or EXIT, Timestamp)
                        signal = SignalEvent(rates[0][0], 'LONG', rates[0][1])
                        self.events.put(signal)
                        self.aped[t] = True


class LongShortMomentumStrategy(Strategy):
    """
    A basic time series momentum (i.e. trend-following) strategy using a 
    lookback window to compute the moving average of the rate. 
    
    Basic strategy as follows:

    1) Convert liquidity indices to APYs
    IF trade_trend set:
        2) Compute the individual (log) relative change in APY at each timestamp: log(dAPY) ~ log(APY(t)/APY(t-1))
        3) Computing moving average of this change over a lookback window, S = EWMA[log(dAPY)] from time t
        4) Current APY trend > S + buffer => LONG; current APY trend < S - buffer => LONG; else EXIT position with the execution handler  
    ELSE:
        2) We just form the same signals as above but with the APY value rather than its trend
    """

    def __init__(self, rates, events, trend_lookback=15, apy_lookback=1, buffer=1, trade_trend=False):
        
        self.rates = rates
        self.token_list = self.rates.token_list
        self.events = events
        self.trend_lookback = trend_lookback
        self.apy_lookback = apy_lookback
        self.buffer = buffer
        self.trade_trend = trade_trend
        self.prior_position = "NONE" # Need to track when positions change

    def calculate_signals(self, event):
        position = self.prior_position # Reset position so that something is always registered at the start of a new event
        if event.type == "MARKET":
            for t in self.token_list:
                liquidity_indexes = self.rates.get_latest_rates(t, N=self.trend_lookback) # List of (token, timestamp, liquidity index) tuples
                rates = self.liquidity_index_to_apy(liquidity_indexes) # Convert to APYs
                trends = self.calculate_trend(rates) # Get trends
                if rates is not None and rates != []:
                    if self.trade_trend:
                        sig, buffer = self.update_beta(series=rates) # ---> Rolling trends
                    else:
                        sig, buffer = self.update_moving_average_and_buffer(series=rates, alpha=0.80) # ---> Rolling rates
                    
                    # Now trade trend or trade rate
                    if self.trade_trend: 
                        if sig > buffer:
                            position = "LONG"
                        elif sig < -buffer:
                            position = "SHORT"
                        else:
                            signal_exit = SignalEvent(liquidity_indexes[-1][0], "EXIT", liquidity_indexes[-1][1])
                            self.events.put(signal_exit)
                    else: 
                        if rates[-1] > sig + buffer:
                            position = "SHORT"
                        elif rates[-1] < sig - buffer:
                            position = "LONG"
                        else:
                            signal_exit = SignalEvent(liquidity_indexes[-1][0], "EXIT", liquidity_indexes[-1][1])
                            self.events.put(signal_exit)
                    
                    if position != self.prior_position:
                        signal = SignalEvent(liquidity_indexes[-1][0], position, liquidity_indexes[-1][1])
                        self.events.put(signal)
                        # Update new prior position to prevent N-counting of the same LONG, SHORT position
                        self.prior_position = position

    def liquidity_index_to_apy(self, rates):
        liq_idx = np.array([r[2] for r in rates])
        timestamps = np.array([r[1] for r in rates])
        apys = []
        for i in range(1, len(liq_idx)):
            window = i-self.apy_lookback if self.apy_lookback<i else 0
            variable_rate = (liq_idx[i]/liq_idx[window]) - 1.0 
            # Annualise the rate
            compounding_periods = SECONDS_IN_YEAR / (timestamps[i] - timestamps[window]).total_seconds()
            apys.append(((1 + variable_rate)**compounding_periods) - 1)
        return np.array(apys)

    @staticmethod
    def calculate_trend(apys):
        return np.array([np.log(apys[i]/apys[i-1]) for i in range(1, len(apys))])

    @staticmethod
    def update_beta(series):
        time_steps = np.arange(len(series)).reshape(-1,1)
        #reg = LinearRegression().fit(time_steps, series.reshape(-1,1))
        reg = RANSACRegressor(random_state=42).fit(time_steps, series.reshape(-1,1))
        beta = reg.estimator_.coef_.flatten()[0]
        return beta, 1e-5

    """
    Calculates the exponential moving average over a vector.
    Will fail for large inputs.
    :param data: Input data
    :param alpha: scalar float in range (0,1)
        The alpha parameter for the moving average.
    :param offset: optional
        The offset for the moving average, scalar. Defaults to data[0].
    :param dtype: optional
        Data type used for calculations. Defaults to float64 unless
        data.dtype is float32, then it will use float32.
    :param order: {'C', 'F', 'A'}, optional
        Order to use when flattening the data. Defaults to 'C'.
    :param out: ndarray, or None, optional
        A location into which the result is stored. If provided, it must have
        the same shape as the input. If not provided or `None`,
        a freshly-allocated array is returned.
    """
    @staticmethod
    def ewma_vectorized(data, alpha, offset=None, dtype=None, order='C', out=None):
        data = np.array(data, copy=False)

        if dtype is None:
            if data.dtype == np.float32:
                dtype = np.float32
            else:
                dtype = np.float64
        else:
            dtype = np.dtype(dtype)

        if data.ndim > 1:
            # flatten input
            data = data.reshape(-1, order)

        if out is None:
            out = np.empty_like(data, dtype=dtype)
        else:
            assert out.shape == data.shape
            assert out.dtype == dtype

        if data.size < 1:
            # empty input, return empty array
            return out

        if offset is None:
            offset = data[0]

        alpha = np.array(alpha, copy=False).astype(dtype, copy=False)

        # scaling_factors -> 0 as len(data) gets large
        # this leads to divide-by-zeros below
        scaling_factors = np.power(1. - alpha, np.arange(data.size + 1, dtype=dtype),
                                   dtype=dtype)
        # create cumulative sum array
        np.multiply(data, (alpha * scaling_factors[-2]) / scaling_factors[:-1],
                dtype=dtype, out=out)
        np.cumsum(out, dtype=dtype, out=out)

        # cumsums / scaling
        out /= scaling_factors[-2::-1]

        if offset != 0:
            offset = np.array(offset, copy=False).astype(dtype, copy=False)
            # add offsets
            out += offset * scaling_factors[1:]

        return out
    
    def update_moving_average_and_buffer(self, series, alpha=0.99):
        series = series[~np.isnan(series)] # Remove NaNs
        moving_average = self.ewma_vectorized(data=series[:-1], alpha=alpha).mean() # Trend up to, but excluding, latest bar
        moving_buffer = self.buffer * self.ewma_vectorized(data=series[:-1], alpha=alpha).std()/np.sqrt(len(series[:-1]))
        return moving_average, moving_buffer
    

class StatisticalArbitragePairs(Strategy):

    def __init__(self, rates, events, pairs, lookback_window=30, apy_lookback=1, deviations=1.0):
        
        self.rates = rates
        self.token_list = self.rates.token_list
        self.events = events
        self.lookback_window = lookback_window # Statistical arbitrage lookback window
        self.apy_lookback = apy_lookback
        self.deviations = deviations # How much to scale the standard deviation of the pairs ratio by
        self.pairs = pairs # The token pairs to arbitrage --> [(token1, token2), (token2, token4), (token1, token4), ...]

    """
        Compute z-score for downstream stat. arb. signal
        construction.
    """
    @staticmethod
    def z_score(series):
        return (series - series.mean())/series.std()

    """
    Generate an APY DataFame from the initial liquidity
    indices
    """
    def liquidity_index_to_apy_df(self, rates, token):
        liq_idx = np.array([r[2] for r in rates])
        timestamps = np.array([r[1] for r in rates])
        apys = []
        for i in range(1, len(liq_idx)):
            window = i-self.apy_lookback if self.apy_lookback<=i else 0
            variable_rate = liq_idx[i]/liq_idx[window] - 1.0 
            # Annualise the rate
            compounding_periods = SECONDS_IN_YEAR / (timestamps[i] - timestamps[window]).total_seconds()
            apys.append(((1 + variable_rate)**compounding_periods) - 1)
        df = pd.DataFrame(data={"Date": timestamps[1:], f"{token} APY": np.array(apys)})
        return df
    
    """
        Take in the z-score and compute the bounds necessary to exit and
        enter positions, which we save as individual signals and run 
        over in the trading engine.
    """
    def calculate_signals(self, event):
        if event.type == "MARKET":
            for pair in self.pairs:
                liq_idx_1 = self.rates.get_latest_rates(pair[0], N=self.lookback_window) 
                liq_idx_2 = self.rates.get_latest_rates(pair[1], N=self.lookback_window) 
                df_1, df_2 = self.liquidity_index_to_apy_df(rates=liq_idx_1, token=pair[0]), \
                    self.liquidity_index_to_apy_df(rates=liq_idx_2, token=pair[1])
                
                # Make sure the pairs share common timestamps, by concatenating
                signals = pd.concat([df_1, df_2], join="inner")
                signals.dropna(inplace=True)
                signals.set_index("Date", inplace=True)

                # Signal construction and analysis
                signals["Ratios"] = signals[f"{pair[0]} APY"]/signals[f"{pair[1]} APY"]

                # Currently using previous month's information, but this might be updated to a more "rolling"
                # arbitrage strategy
                signals["Year"] = np.array([int("-".join(i.split(" ")[0].split("-")[:1])) for i in signals.index])
                signals["Month"] = np.array([int("-".join(i.split(" ")[0].split("-")[1:2])) for i in signals.index])

                # Calculate z-score and define upper and lower thresholds (e.g. 1 standard deviation)
                # Possilve updates: include two-weeks rebalancing, relevant for the initially proposed strategy 
                # For now we don't have a rolling window, but arbitrage only based on the previous month's information
                signals_collected = []
                for y in signals["Year"].unique(): 
                    for m in signals["Month"].unique():
                        (y_prev, m_prev) = (y-1, 12) if m==1 else (y, m-1)
                        signals_prev = signals.loc[lambda signals: (signals["Year"]== y_prev) & (signals["Month"]==m_prev)]
                        signals_temp = signals.loc[lambda signals: (signals["Year"]== y) & (signals["Month"]==m)]
                        signals_temp["Z"] = self.z_score(signals_prev["Ratios"])
                        signals_temp["Z upper limit"] = signals_temp["Z"].mean() + signals_temp["Z"].std()*self.deviations
                        signals_temp["Z lower limit"] = signals_temp["Z"].mean() - signals_temp["Z"].std()*self.deviations
        
                        signals_collected.append(signals_temp)
                del signals
                
                # Make complete set of signals
                signals_all = pd.concat(signals_collected)
        
                # Create signal - short if Z-score is greater than upper limit else long
                signals_all["Signals 1"] = 0
                signals_all["Signals 1"] = np.select([signals_all["Z"] > \
                                 signals_all["Z upper limit"], signals_all["Z"] < signals_all["Z lower limit"]], [-1, 1], default=0)
                # We take the first order difference to obtain portfolio position in that given coin
                signals_all["Positions 1"] = signals_all["Signals 1"].diff()
        
                # Repeat for the second coin signal
                signals_all["Signals 2"] = -signals_all["Signals 1"]
                signals_all["Positions 2"] = signals_all["Signals 2"].diff()

                # Send the different positions to the execution handler
                # Just need to use replace to get LONG, SHORT, EXIT
                # Then send to execution handler    
                signals_all.replace(
                                {
                                    "Positions 1": {1: "LONG", -1: "SHORT", 0: "EXIT"},
                                    "Positions 2": {1: "LONG", -1: "SHORT", 0: "EXIT"},
                                }
                )
                signal1 = SignalEvent(liq_idx_1[-1][0], signals_all["Positions 1"].iloc[-1], liq_idx_1[-1][1])
                signal2 = SignalEvent(liq_idx_2[-1][0], signals_all["Positions 2"].iloc[-1], liq_idx_2[-1][1])
                self.events.put(signal1)
                self.events.put(signal2)

    """
        Compute the fortnights for bi-monthly rebalancing
    """
    @staticmethod
    def get_fortnight(df):
        fnight = []
        num = 1
        counter = 0
        for i in range(len(df)):
            counter += 1
            if counter > 14:
                counter = 0
            num += 1
            fnight.append(num)
        df["Fnight"] = fnight  

    """
    Some additional methods to investigate if different pairs are coinintegated,
    and the associated statistical significances. These are not part of the signal-forming
    strategy managed by the execution handler downstream, but they can be called
    from the Stat Arb class for initial investigations between the pairs. Need to provide
    a DataFrame of the APYs to investigate this, together with stating and ending
    timestamps. The DataFrame can contain multiple APY series, and all pairwise 
    combinations are investigated.
    """

    def plot_coint_p_values(self, df, term_start="2020-02-29", term_end="2020-03-29"):

        df = df.loc[lambda df: (df["Date"] > term_start) & (df["Date"] < term_end)]
        pvalues, pairs = self.find_cointegrated_pairs(df=df)

        fig, ax = plt.subplots(figsize=(10, 7))
        sns.heatmap(pvalues, xticklabels=df.columns, yticklabels=df.columns, \
            cmap='RdYlGn_r', annot=True, fmt=".2f", mask=(pvalues >= 0.99))

        ax.set_title('Rates Cointregration Matrix p-values Between Pairs')
        plt.tight_layout()
        if not os.path.exists("./stat_arb_tests"):
            os.makedirs("./stat_arb_tests")
        plt.savefig(f'stat_arb_tests/cointegrated_pairs_{term_start}_{term_end}.png', dpi=300)
        plt.close()

    """
    Pairwise conintegration tests on an APY DataFrame
    """
    @staticmethod
    def find_cointegrated_pairs(df):
        
        n = df.shape[1]
        p_value_matrix = np.ones((n, n))
        keys = df.columns.tolist()
        pairs = []

        for i in range(n):
            for j in range(i+1, n):

                series_1 = df.loc[:, keys[i]]
                series_2 = df.loc[:, keys[j]]

                result = coint(series_1, series_2)
                p_value_matrix[i, j] = result[1]
                if result[1] < 0.05:
                    pairs.append((keys[i], keys[j]))

        return p_value_matrix, pairs
    
    """
    Pairwise (Pearson) correlation matrix
    """
    @staticmethod
    def plot_correlation_matrix(df, term_start="2020-02-29", term_end="2020-03-29"):

        df = df.loc[lambda df: (df["Date"] > term_start) & (df["Date"] < term_end)]
        fig, ax = plt.subplots(figsize=(10, 7))
        sns.heatmap(df.corr(method="pearson"), ax=ax, cmap="coolwarm", annot=True, fmt=".2f")  

        ax.set_title('Rates Correlation Matrix')
        plt.tight_layout()
        if not os.path.exists("./stat_arb_tests"):
            os.makedirs("./stat_arb_tests")
        plt.savefig(f"stat_arb_tests/correlation_{term_start}_{term_end}.png", dpi=300)
        plt.close()


    def perform_stationarity_test(self, df, term_start="2020-02-29", term_end="2020-03-29", label=" APY"):
        
        df = df.loc[lambda df: (df["Date"] > term_start) & (df["Date"] < term_end)]
        for pair in self.pairs:
            train = pd.DataFrame()
            train['token_1'] = df.loc[:, pair[0]+label]
            train['token_2'] = df.loc[:, pair[1]+label]

            # Visualize rates
            ax = train[["token_1", "token_2"]].plot(figsize=(12, 6), title = f"Rates for {pair[0]} and {pair[1]}")
            ax.set_ylabel("Rate")
            ax.grid(True)
            if not os.path.exists("./stat_arb_tests"):
                os.makedirs("./stat_arb_tests")
            plt.savefig(f"stat_arb_tests/rates_{term_start}_{term_end}_{pair[0]}_{pair[1]}.png", dpi=300)
            plt.close()

            # Run OLS
            model = sm.OLS(train["token_1"], train["token_2"]).fit()

            # Regression summary results
            plt.rc('figure', figsize=(12, 7))
            plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 16}, fontproperties='monospace')
            plt.axis('off')
            plt.tight_layout()
            plt.subplots_adjust(left=0.2, right=0.8, top=0.7, bottom=0.1)
            plt.savefig(f"stat_arb_tests/OLS_results_{term_start}_{term_end}_{pair[0]}_{pair[1]}.png", dpi=300)
            plt.close()

            print("Hedge Ratio = ", model.params[0])

            # Calculate spread
            spread = train['coin_1'] - model.params[0] * train['coin_2']

            # Plot spread
            ax = spread.plot(figsize=(12, 6), title="Rates Spread")
            ax.set_ylabel("Spread")
            ax.grid(True)
            plt.savefig(f'stat_arb_tests/spread_{term_start}_{term_end}_{pair[0]}_{pair[1]}', dpi=300)
            plt.close()

            # Conduct Augmented Dickey-Fuller test
            adf = adfuller(spread, maxlag=1)
            print("Critical Value = ", adf[0])

            # Critical values
            print("ADF critical values: ", adf[4])